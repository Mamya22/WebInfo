# Web信息处理与应用 实验一


## 实验简介

​	结合给定的电影，书籍的标签信息，实现对电影和书籍的检索，在索引的基础上，结合用户评价信息及用户间的社交关系，进行个性化电影和书籍推荐

### 实验环境

+ System：Win 11
+ 开发工具：Vscode，Pycharm community
+ 编程语言：python
+ 编程环境：Anaconda

+ ==TODO==

### 实验成员

+ 组长：方馨   PB22111656
+ 组员：马筱雅 PB22111639
+ 组员：陈昕琪 PB22111711

## 实验内容

### 第一阶段 豆瓣数据的索引

#### 1. 对数据进行预处理

+ 两种分词方法说明，使用jieba和SnowNLP两种分词工具。  
  + jieba：支持三种分词模式：
  精确模式，试图将句子最精确地切开，适合文本分析；
  全模式，全是一种比较宽松的分词模式，它会将文本中所有可能的词语都分出来，速度非常快，但是不能解决歧义（不考虑这种模式）；
  搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。
  + SnowNLP：一个用python写的类库，可用于中文分词，词性标注，情感分析等。
+ 两种分词方法比较：
  + 在去除停用词的情况下，直观比较分词文件结果大小可以发现用jieba的精确模式文件大小为4546kb，jieba的搜索引擎模式文件大小为5269kb，SnowNLP文件大小为5090kb。  
  + 进一步对比发现，SnowNLP倾向于把词分的更短更细致，比如对于“不象话”这个词，SnowNLP把它分成了"不象"和"话"两个词。而jieba的精确模式倾向于将词语精确地切开，分的是“不象话”这个完整的词。jieba的搜索引擎模式则会保留多种分割结果，分的是“不象”“象话”和“不象话”三个词。还有jieba的精确模式分词“村上春树”，jieba的搜索引擎模式分为“村上”和“村上春树”，而SnowNLP分词“村”“上”“春”“树”等等，这样的例子还有很多。在这个布尔检索项目中，jieba的搜索引擎模式一方面对于不确定的词语保留了分词的多种可能，对比精简模式有更高的召回率，又不像SnowNLP，把词语分的过于琐碎，有些词语失去了完整性。我认为jieba的搜索引擎模式分词召回率更高且词语更为完整。因此选择jieba的搜索引擎模式完成接下来的实验部分。但是值得注意的是，在后续合并近义词时，多种分词结果可能会被当做近义词合并，将相似度设置为较高的0.9可以避免一部分这种情况，但仍值得思考。
+ 去除停用词
  + 选用哈工大停用词表，去除词项中的停用词。
  + 不做处理的分词的未压缩的书籍倒排索引表文件大小4296kb，压缩后的书籍倒排索引表文件大小487kb，不做处理的分词的未压缩的电影倒排索引表文件大小14176kb，压缩后的电影倒排索引表文件大小1385kb。去除停用词后的未压缩的书籍倒排索引表文件大小4012kb，压缩后的书籍倒排索引表文件大小454kb，去除停用词后的未压缩的电影倒排索引表文件大小13458kb，压缩后的电影倒排索引表文件大小1316kb。去除停用词使压缩后的倒排索引表文件大小减小为95%左右。
+ 合并近义词
  
  核心代码如下：

  ```python
  for i in range(len(extracted_word)):
      if extracted_word[i] not in self.single_id_info and extracted_word[i] != ' ':#不在列表中的词加入
          flag = 0
          embeddings1 = model.encode(extracted_word[i])
          for j in range(l):#若和列表中的词词意相近则删除，否则加入
              embeddings2 = self.embeddings[j]
              if model.similarity(embeddings1,embeddings2).item() > 0.9:
                  flag = 1
                  break
          if flag == 0:
              self.single_id_info.append(extracted_word[i])
              self.embeddings.append(embeddings1)
              l += 1
  ```

  + 使用sentence_transformers库分析两个词语的相似度，遍历某本书或某个电影的所有词项，相似度高于0.9的合并。
  + 去除停用词且合并近义词后的未压缩的书籍倒排索引表文件大小2656kb，压缩后的书籍倒排索引表文件大小339kb，去除停用词后且合并近义词的未压缩的电影倒排索引表文件大小8331kb，压缩后的电影倒排索引表文件大小927kb。去除停用词且合并近义词对比仅去除停用词，使压缩后的倒排索引表文件大小减小为70%左右，提升效果显著。

#### 2. 建立倒排索引表和跳表指针

+ 倒排表的建立

  遍历预处理生成的`keyword`数据集，如果是新的词项则创建列表，如果已经存在，则将其按照`ID`升序的顺序插入到对应的倒排表中

  核心代码如下：

  ```python
  for key, items in self.dict.items():
      for item in items:
          key_value = int(key)
          if key_value not inself.reverted_dict[item]:
               bisect.insort(self.reverted_dict[item], key_value) # 按照顺序插入
  ```

+ 跳表指针的生成

  对于已经实现的倒排索引表，遍历所有的关键词，为每个关键词生成跳表，根据其倒排表的长度计算跳表的间隔并生成跳表节点。

  **这里用$\sqrt{L}$作为跳表间隔，其中L为倒排表长度**

  ==TODO：加图片，数据分析==

#### 3. 布尔查询

+ 基本操作符

  + AND

    对两个倒排表求交集，首先根据`tag`获取到两个倒排表和对应的跳表。在两个表的开头分别放置指针，如果两个倒排表的当前 ID 相等，则将该 ID 添加到结果列表 `ret` 中，并同时增加两个索引。如果不相等，则增加较小 ID 所在的列表的索引。

    其中跳跃通过循环实现，如果当前节点的 ID 在第一个列表中小于第二个列表中，且跳跃后的 ID 也小于，则跳过多个节点。

  + OR

    对两个倒排表求并集，获取到两个倒排表和对应的跳表。如果两个倒排表的当前 ID 相等，则将该 ID 添加到结果列表 `ret` 中，并同时增加两个索引。如果不相等，则增加较小 ID 所在的列表的索引，并将该 ID 添加到结果列表 `ret` 中。最后对剩余元素进行处理，将元素添加到结果列表中。

  + AND NOT

    从第一个倒排表中移除出现在第二个倒排表中的文档ID，即求差集。如果两个倒排表的当前 ID 相等，则同时增加两个索引。 如果不相等，则增加较小 ID 所在的列表的索引，并将该 ID 添加到结果列表 `ret` 中。最后如果第一个倒排表还有剩余元素，则添加到结果列表中。

  + NOT

    从预先存储的文档ID列表中去掉传入的倒排表中的所有ID，利用AND_NOT方法，即可实现

+ 文法输入设计

  输入允许括号，`AND`，`OR`，`NOT`四种运算符，实现的文法如下

  ```python
  <expression> ::= <term> | <expression> "OR" <term>
  <term> ::= <factor> | <term> "AND" <factor> | <term> " AND NOT" <factor>
  <factor> ::= "NOT" <factor> | "(" <expression> ")" | <keyword>
  <keyword> ::= "tag" # 倒排表中的tag
  ```

  其中，`<expression>` 是表达式，可以是一个 `<term>` 或者是 `<expression>` 和 `"OR"` 连接的 `<term>`。

  `<term>` 表示项，可以是一个 `<factor>`、`<term>` 和 `"AND"` 连接的 `<factor>`，或 `<term>` 和 `"AND NOT"` 连接的 `<factor>`。

  `<factor>` 表示因子，可以是 `"NOT"` 连接的 `<factor>`、括号括起来的 `<expression>`，或者是一个 `<keyword>`。

  `<keyword>` 表示关键词，即倒排表中的关键词。

  当输入时，首先对其进行归一化处理，排查括号的中英文是否正确或者大小写中英文，全部转化为英文括号和大写操作符。之后先递归的匹配左括号，找到对应的右括号之后，再递归寻找，直到所有的括号都被拆除后，根据递归顺序匹配字符串，进行操作。最后返回找到的`ID`，然后输出`ID`值和`ID`包含的全部`tag`值

+ 输入输出窗口

  ==TODO 数据展示，分析==

#### 4. 实现索引压缩



### 第二阶段 豆瓣数据的个性化检索与推荐

#### 通过协同过滤进行评分预测



#### 根据`tag`等文本信息辅助预测



## 实验总结



